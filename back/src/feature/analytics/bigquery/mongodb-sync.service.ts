import { bigqueryClient } from './bigquery-client';
import { TABLE_SCHEMAS } from './table-schemas';
import db from 'models';
import { log } from '../../../utils/logger/logger';
import { cacheProvider } from '../../../cache';

export class MongodbSyncService {
  private readonly BATCH_SIZE = 1000;

  private readonly DATASET_ID = process.env.GA4_DATASET_FOUNDATION_ID!;

  /**
   * í…Œì´ë¸” ìë™ ìƒì„± í•¨ìˆ˜
   */
  private async ensureTableExists(tableName: string) {
    try {
      console.log(`ğŸ” Checking table ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });

      const table = dataset.table(tableName);
      const [exists] = await table.exists();

      if (!exists) {
        const schema = TABLE_SCHEMAS[tableName];
        if (!schema) {
          throw new Error(`Schema not found for table: ${tableName}`);
        }

        console.log(`ğŸ—ï¸ Creating table ${this.DATASET_ID}.${tableName} with schema...`);
        await table.create({
          schema: schema,
          location: 'asia-northeast3',
        });
        console.log(`âœ… Table ${this.DATASET_ID}.${tableName} created successfully`);
      } else {
        console.log(`âœ… Table ${this.DATASET_ID}.${tableName} already exists`);
      }
    } catch (error) {
      console.error(`âŒ Failed to create table ${this.DATASET_ID}.${tableName}:`, error);
      throw error;
    }
  }

  /**
   * í…Œì´ë¸” ë°ì´í„°ë§Œ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
   */
  private async clearTableData(tableName: string) {
    try {
      console.log(`ğŸ—‘ï¸ Clearing data from table ${this.DATASET_ID}.${tableName}...`);

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });
      const table = dataset.table(tableName);

      // í…Œì´ë¸” ì¡´ì¬ í™•ì¸
      const [exists] = await table.exists();

      if (exists) {
        // ë°ì´í„°ë§Œ ì‚­ì œ (í…Œì´ë¸” êµ¬ì¡°ëŠ” ìœ ì§€)
        const query = `DELETE FROM \`${this.DATASET_ID}.${tableName}\` WHERE TRUE`;
        const [job] = await bigqueryClient.createQueryJob({
          query: query,
          location: 'asia-northeast3',
        });

        await job.getQueryResults();
        console.log(`âœ… Cleared data from table ${this.DATASET_ID}.${tableName}`);
      } else {
        // í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        const schema = TABLE_SCHEMAS[tableName];
        if (!schema) {
          throw new Error(`Schema not found for table: ${tableName}`);
        }

        await table.create({
          schema: schema,
          location: 'asia-northeast3',
        });
        console.log(`âœ… Created table ${this.DATASET_ID}.${tableName}`);
      }
    } catch (error) {
      console.error(`âŒ Failed to clear table ${tableName}:`, error);
      throw error;
    }
  }

  /**
   * ëª¨ë“  ë°ì´í„°ë¥¼ ì¦ë¶„ ë™ê¸°í™”
   */
  async syncAllData() {
    try {
      log.info('MongoDB ë™ê¸°í™” ì‹œì‘', 'SCHEDULER', 'LOW');

      // jobs.tsì—ì„œ ì •ì˜ëœ MongoDB ë™ê¸°í™” ì‘ì—…ë“¤ë§Œ ì‹¤í–‰
      const { mongodbSyncJobs } = await import('../scheduler/jobs');

      const lastSync = await this.getLastSyncTime();
      const isFullSync = !lastSync; // ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ì´ ì—†ìœ¼ë©´ ì „ì²´ ë™ê¸°í™”

      if (isFullSync) {
        log.info('ì „ì²´ ë™ê¸°í™” ëª¨ë“œ: ëª¨ë“  ë°ì´í„°ë¥¼ ìƒˆë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤', 'SCHEDULER', 'LOW');
      } else {
        log.info(
          `ì¦ë¶„ ë™ê¸°í™” ëª¨ë“œ: ${lastSync.toISOString()} ì´í›„ ë°ì´í„°ë§Œ ë™ê¸°í™”í•©ë‹ˆë‹¤`,
          'SCHEDULER',
          'LOW'
        );
      }

      // ê° ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
      for (const job of mongodbSyncJobs) {
        if (job.type === 'mongodb_sync') {
          console.log(`ğŸ”„ Processing ${job.name}...`);

          // í…Œì´ë¸” ì¡´ì¬ í™•ì¸
          await this.ensureTableExists(job.destinationTable);

          if (isFullSync) {
            // ì „ì²´ ë™ê¸°í™”: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì „ì²´ ë°ì´í„° ë™ê¸°í™”
            await this.clearTableData(job.destinationTable);
            await this.syncCollection(job.collection!, job.destinationTable);
          } else {
            // ì¦ë¶„ ë™ê¸°í™”: ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì´í›„ ë°ì´í„°ë§Œ ë™ê¸°í™”
            await this.syncCollection(job.collection!, job.destinationTable, lastSync);
          }

          console.log(`âœ… ${job.name} ì™„ë£Œ`);
        }
      }

      await this.updateLastSyncTime(new Date());

      log.info('MongoDB ë™ê¸°í™” ì™„ë£Œ', 'SCHEDULER', 'LOW');
    } catch (error) {
      void log.error('MongoDB ë™ê¸°í™” ì‹¤íŒ¨', 'SCHEDULER', 'HIGH', { error });
      throw error;
    }
  }

  /**
   * ë™ì ìœ¼ë¡œ ì»¬ë ‰ì…˜ ë™ê¸°í™”
   */
  private async syncCollection(collectionName: string, tableName: string, lastSyncTime?: Date) {
    const query = lastSyncTime ? { updatedAt: { $gt: lastSyncTime } } : {};

    // ì¦ë¶„ ë™ê¸°í™”ì¸ ê²½ìš° ë¡œê·¸ ì¶œë ¥
    if (lastSyncTime) {
      log.info(
        `${collectionName} ì¦ë¶„ ë™ê¸°í™” ì‹œì‘: ${lastSyncTime.toISOString()} ì´í›„ ë°ì´í„°`,
        'SCHEDULER',
        'LOW'
      );
    } else {
      log.info(`${collectionName} ì „ì²´ ë™ê¸°í™” ì‹œì‘`, 'SCHEDULER', 'LOW');
    }

    let skip = 0;
    let hasMore = true;
    let totalProcessed = 0;

    while (hasMore) {
      let data: any[];

      // ì»¬ë ‰ì…˜ë³„ë¡œ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
      switch (collectionName) {
        case 'users':
          data = await db.User.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        case 'purchases':
          data = await db.Purchase.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        case 'purchase_failures':
          data = await db.PurchaseFailure.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        case 'requests':
          data = await db.Request.find(query).skip(skip).limit(this.BATCH_SIZE).lean();
          break;
        default:
          throw new Error(`Unknown collection: ${collectionName}`);
      }

      if (data.length === 0) {
        hasMore = false;
        break;
      }

      // ì»¬ë ‰ì…˜ë³„ë¡œ ë‹¤ë¥¸ ë³€í™˜ ë¡œì§ ì ìš©
      const transformedData = this.transformData(collectionName, data);

      await this.insertBatchToBigQuery(tableName, transformedData, !!lastSyncTime);

      totalProcessed += data.length;
      skip += this.BATCH_SIZE;

      // ì§„í–‰ ìƒí™© ë¡œê·¸
      if (totalProcessed % (this.BATCH_SIZE * 5) === 0) {
        log.info(
          `${collectionName} ë™ê¸°í™” ì§„í–‰: ${totalProcessed}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ë¨`,
          'SCHEDULER',
          'LOW'
        );
      }
    }

    log.info(
      `${collectionName} ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ (ì´ ${totalProcessed}ê°œ ë ˆì½”ë“œ)`,
      'SCHEDULER',
      'LOW'
    );
  }

  /**
   * ì»¬ë ‰ì…˜ë³„ ë°ì´í„° ë³€í™˜
   */
  private transformData(collectionName: string, data: any[]): any[] {
    switch (collectionName) {
      case 'users':
        return data.map((user) => ({
          _id: user._id.toString(),
          email: user.email,
          point: Number(user.point) || 0,
          aiPoint: Number(user.aiPoint) || 0,
          level: Number(user.level) || 1,
          lastLoginAt: user.lastLoginAt?.toISOString() || null,
          MembershipAt: user.MembershipAt?.toISOString() || null,
          lastMembershipAt: user.lastMembershipAt?.toISOString() || null,
          event: user.event || null,
          createdAt: user.createdAt?.toISOString() || null,
          updatedAt: user.updatedAt?.toISOString() || null,
        }));

      case 'purchases':
        return data.map((purchase) => ({
          _id: purchase._id.toString(),
          userId: purchase.userId.toString(),
          productId: purchase.product?.productId || null,
          platform: purchase.product?.platform || null,
          type: purchase.product?.type || null,
          isExpired: purchase.isExpired || false,
          createdAt: purchase.createdAt.toISOString(),
          updatedAt: purchase.updatedAt.toISOString(),
        }));

      case 'purchase_failures':
        return data.map((failure) => ({
          _id: failure._id.toString(),
          userId: failure.userId?.toString() || null,
          productId: failure.productId?.toString() || null,
          status: failure.status || null,
          platform: failure.platform || null,
          createdAt: failure.createdAt.toISOString(),
          updatedAt: failure.updatedAt.toISOString(),
        }));

      case 'requests':
        return data.map((request) => ({
          _id: request._id.toString(),
          userId: request.userId?.toString() || null,
          status: request.status || null,
          type: request.type || null,
          name: request.name || null,
          text: request.text || null,
          product: request.product ? JSON.stringify(request.product) : null,
          review: request.review ? JSON.stringify(request.review) : null,
          answer: request.answer ? JSON.stringify(request.answer) : null,
          createdAt: request.createdAt?.toISOString() || null,
          updatedAt: request.updatedAt?.toISOString() || null,
        }));

      default:
        throw new Error(`Unknown collection: ${collectionName}`);
    }
  }

  /**
   * BigQueryì— ë°°ì¹˜ ë°ì´í„° ì‚½ì… (ì¦ë¶„ ë™ê¸°í™” ì‹œ UPSERT ì‚¬ìš©)
   */
  private async insertBatchToBigQuery(tableName: string, data: any[], isIncremental = false) {
    if (data.length === 0) return;

    try {
      console.log(
        `ğŸ“Š ${isIncremental ? 'Upserting' : 'Inserting'} data to ${this.DATASET_ID}.${tableName}...`
      );

      const dataset = bigqueryClient.dataset(this.DATASET_ID, {
        location: 'asia-northeast3',
      });
      const table = dataset.table(tableName);

      if (isIncremental) {
        // ì¦ë¶„ ë™ê¸°í™”: UPSERT ì‚¬ìš© (MERGE ì¿¼ë¦¬)
        await this.upsertData(tableName, data);
      } else {
        // ì „ì²´ ë™ê¸°í™”: ì¼ë°˜ INSERT
        await table.insert(data);
      }

      console.log(
        `âœ… ${isIncremental ? 'Upserted' : 'Inserted'} ${data.length} records to ${this.DATASET_ID}.${tableName}`
      );
    } catch (error) {
      console.error(
        `âŒ Failed to ${isIncremental ? 'upsert' : 'insert'} data to ${this.DATASET_ID}.${tableName}:`,
        error
      );
      throw error;
    }
  }

  /**
   * UPSERTë¥¼ ìœ„í•œ MERGE ì¿¼ë¦¬ ì‹¤í–‰
   */
  private async upsertData(tableName: string, data: any[]) {
    if (data.length === 0) return;

    // ì„ì‹œ í…Œì´ë¸”ì— ë°ì´í„° ì‚½ì… í›„ MERGE ì‹¤í–‰
    const tempTableName = `${tableName}_temp_${Date.now()}`;
    const dataset = bigqueryClient.dataset(this.DATASET_ID, {
      location: 'asia-northeast3',
    });

    try {
      // 1. ì„ì‹œ í…Œì´ë¸” ìƒì„± ë° ë°ì´í„° ì‚½ì…
      const tempTable = dataset.table(tempTableName);
      await tempTable.create({
        schema: TABLE_SCHEMAS[tableName],
      });

      await tempTable.insert(data);

      // 2. MERGE ì¿¼ë¦¬ ì‹¤í–‰
      const columns = Object.keys(data[0]);
      const updateColumns = columns.filter((key) => key !== '_id');

      const mergeQuery = `
        MERGE \`${this.DATASET_ID}.${tableName}\` AS target
        USING \`${this.DATASET_ID}.${tempTableName}\` AS source
        ON target._id = source._id
        WHEN MATCHED THEN
          UPDATE SET
            ${updateColumns.map((key) => `${key} = source.${key}`).join(',\n            ')}
        WHEN NOT MATCHED THEN
          INSERT (${columns.join(', ')})
          VALUES (${columns.map((key) => `source.${key}`).join(', ')})
      `;

      const [job] = await bigqueryClient.createQueryJob({
        query: mergeQuery,
        location: 'asia-northeast3',
      });

      await job.getQueryResults();

      // 3. ì„ì‹œ í…Œì´ë¸” ì‚­ì œ
      await tempTable.delete();
    } catch (error) {
      // ì„ì‹œ í…Œì´ë¸” ì •ë¦¬
      try {
        const tempTable = dataset.table(tempTableName);
        await tempTable.delete();
      } catch (cleanupError) {
        console.warn('ì„ì‹œ í…Œì´ë¸” ì •ë¦¬ ì‹¤íŒ¨:', cleanupError);
      }
      throw error;
    }
  }

  /**
   * ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì¡°íšŒ
   */
  private async getLastSyncTime(): Promise<Date | undefined> {
    const lastSync = cacheProvider.get<string>('mongodb_last_sync_time');
    console.log('ğŸ” MongodbSyncService.getLastSyncTime:', {
      lastSync,
      isUndefined: lastSync === undefined,
      cacheProvider: cacheProvider.constructor.name,
    });
    return lastSync ? new Date(lastSync) : undefined;
  }

  /**
   * ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸
   */
  private async updateLastSyncTime(time: Date) {
    // ìºì‹œì— ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì €ì¥ (TTL ì—†ì´ ì˜êµ¬ ì €ì¥)
    cacheProvider.set('mongodb_last_sync_time', time.toISOString(), 0);
    void log.info(`ë§ˆì§€ë§‰ ë™ê¸°í™” ì‹œê°„ ì—…ë°ì´íŠ¸: ${time.toISOString()}`, 'SCHEDULER', 'LOW');
  }
}

export const mongodbSyncService = new MongodbSyncService();
