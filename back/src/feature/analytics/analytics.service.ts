// src/features/analytics/analytics.service.ts

import fs from 'fs/promises';
import path from 'path';
import { bigqueryClient } from './bigquery/bigquery-client';
import { MetricJob } from './types/types';
import { TABLE_SCHEMAS } from './bigquery/table-schemas';
import { log } from '../../utils/logger/logger';

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID;
const RAW_DATASET = process.env.GA4_DATASET_RAW_ID; // ì›ë³¸ GA4 ë°ì´í„°ì…‹
const FOUNDATION_DATASET = process.env.GA4_DATASET_FOUNDATION_ID; // ì¤‘ê°„ ë°ì´í„°ì…‹ ì´ë¦„ìœ¼ë¡œ ë³€ê²½

async function updateTableSchema(datasetId: string, tableName: string) {
  try {
    const dataset = bigqueryClient.dataset(datasetId, {
      location: 'asia-northeast3',
    });

    const table = dataset.table(tableName);
    const [exists] = await table.exists();

    if (!exists) {
      console.log(`Table ${datasetId}.${tableName} does not exist, skipping schema update`);
      return;
    }

    // í˜„ì¬ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê°€ì ¸ì˜¤ê¸°
    const [metadata] = await table.getMetadata();
    const currentSchema = metadata.schema?.fields || [];
    const currentFieldNames = new Set(currentSchema.map((field: any) => field.name));

    // ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ ì •ì˜
    const newSchema = TABLE_SCHEMAS[tableName];
    if (!newSchema) {
      console.log(`No schema definition found for table: ${tableName}`);
      return;
    }

    // ì¶”ê°€í•  í•„ë“œë“¤ ì°¾ê¸°
    const fieldsToAdd = newSchema.filter((field) => !currentFieldNames.has(field.name));

    if (fieldsToAdd.length === 0) {
      console.log(`âœ… Table ${datasetId}.${tableName} schema is up to date`);
      return;
    }

    console.log(
      `ğŸ”„ Adding ${fieldsToAdd.length} new fields to ${datasetId}.${tableName}:`,
      fieldsToAdd.map((f) => f.name)
    );

    // ALTER TABLE ì¿¼ë¦¬ ìƒì„±
    const alterQueries = fieldsToAdd.map((field) => {
      const fieldType =
        field.type === 'FLOAT'
          ? 'FLOAT64'
          : field.type === 'INTEGER'
            ? 'INT64'
            : field.type === 'STRING'
              ? 'STRING'
              : field.type === 'DATE'
                ? 'DATE'
                : field.type === 'TIMESTAMP'
                  ? 'TIMESTAMP'
                  : field.type === 'BOOLEAN'
                    ? 'BOOL'
                    : field.type === 'JSON'
                      ? 'JSON'
                      : field.type;

      return `ADD COLUMN ${field.name} ${fieldType}`;
    });

    const alterQuery = `ALTER TABLE \`${PROJECT_ID}.${datasetId}.${tableName}\` ${alterQueries.join(', ')}`;

    console.log(`[DEBUG] ALTER TABLE query: ${alterQuery}`);

    // ALTER TABLE ì‹¤í–‰
    const [queryJob] = await bigqueryClient.createQueryJob({
      query: alterQuery,
    });

    await queryJob.getQueryResults();
    console.log(`âœ… Successfully updated schema for ${datasetId}.${tableName}`);
  } catch (error) {
    console.error(`âŒ Failed to update schema for ${datasetId}.${tableName}:`, error);
    throw error;
  }
}

// í…Œì´ë¸” ìë™ ìƒì„± í•¨ìˆ˜
async function ensureTableExists(datasetId: string, tableName: string) {
  try {
    const dataset = bigqueryClient.dataset(datasetId, {
      location: 'asia-northeast3',
    });

    const table = dataset.table(tableName);
    const [exists] = await table.exists();

    if (!exists) {
      const schema = TABLE_SCHEMAS[tableName];
      if (!schema) {
        throw new Error(`Schema not found for table: ${tableName}`);
      }

      await table.create({
        schema: schema,
        location: 'asia-northeast3',
      });
      console.log(`âœ… Table ${datasetId}.${tableName} created automatically`);
    } else {
      await updateTableSchema(datasetId, tableName);
    }
  } catch (error) {
    console.error(`âŒ Failed to create table ${datasetId}.${tableName}:`, error);
    throw error;
  }
}

// [ë³€ê²½] destinationDatasetì„ ì¸ìë¡œ ë°›ë„ë¡ ìœ ì§€ (í•¸ë“¤ëŸ¬ì—ì„œ ì œì–´)
export const runEtlJob = async (
  job: MetricJob,
  destinationDataset: string,
  targetDate?: string
) => {
  const queryParams = job.getQueryParams ? job.getQueryParams() : {};

  // targetDateê°€ ì œê³µë˜ë©´ í•´ë‹¹ ë‚ ì§œë¥¼ ì‚¬ìš©, ì•„ë‹ˆë©´ jobì˜ ê¸°ë³¸ê°’ ì‚¬ìš©
  if (targetDate) {
    queryParams.target_date = targetDate;
  }

  const jobDateForLog = queryParams.target_date || new Date().toISOString().split('T')[0];

  console.log(`[START] Running ETL job: ${job.name} for ${jobDateForLog}`);
  console.log(
    `[DEBUG] Environment variables: PROJECT_ID=${PROJECT_ID}, RAW_DATASET=${RAW_DATASET}, FOUNDATION_DATASET=${FOUNDATION_DATASET}`
  );

  try {
    // í…Œì´ë¸” ìë™ ìƒì„±
    await ensureTableExists(destinationDataset, job.destinationTable);

    const queryPath = path.join(__dirname, `./queries/${job.sqlFile}`);
    const sqlTemplate = await fs.readFile(queryPath, 'utf-8');

    // ë™ì ìœ¼ë¡œ í…Œì´ë¸” ê²½ë¡œ ìƒì„±
    const destinationTablePath = `\`${PROJECT_ID}.${destinationDataset}.${job.destinationTable}\``;
    const rawEventsTablePath = `\`${PROJECT_ID}.${RAW_DATASET}.events_*\``;
    const foundationDatasetPath = `\`${PROJECT_ID}.${FOUNDATION_DATASET}\``; // foundation í…Œì´ë¸”ë“¤ì„ ì°¸ì¡°í•  ë•Œ ì‚¬ìš©

    // SQL í…œí”Œë¦¿ì˜ í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜
    const sqlBody = sqlTemplate
      .replace(/{{- GA4_EVENTS_TABLE -}}/g, rawEventsTablePath)
      .replace(/{{- FOUNDATION_DATASET -}}/g, foundationDatasetPath)
      .replace(/{{- DESTINATION_TABLE -}}/g, destinationTablePath); // MERGEë¬¸ì—ì„œ ëª©ì ì§€ í…Œì´ë¸”ì„ ì°¸ì¡°í•  ë•Œ ì‚¬ìš©

    let finalQuery = '';
    const disposition = job.writeDisposition || 'WRITE_APPEND'; // ê¸°ë³¸ê°’ì€ INSERT

    // [ê°œì„ ] Jobì— ì •ì˜ëœ writeDispositionì— ë”°ë¼ ìµœì¢… ì¿¼ë¦¬ ìƒì„±
    if (disposition === 'MERGE') {
      finalQuery = sqlBody; // MERGEë¬¸ì€ SQL íŒŒì¼ì— ì™„ì„±ëœ í˜•íƒœë¡œ ì‘ì„±
    } else {
      // 'WRITE_APPEND' (ê¸°ë³¸ê°’)
      finalQuery = `INSERT INTO ${destinationTablePath} ${sqlBody}`;
    }

    // ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶”ê°€
    console.log(`[DEBUG] destinationTablePath: ${destinationTablePath}`);
    console.log(`[DEBUG] finalQuery: ${finalQuery.substring(0, 200)}...`);

    const [queryJob] = await bigqueryClient.createQueryJob({
      query: finalQuery,
      params: queryParams,
    });
    await queryJob.getQueryResults();
    console.log(`[SUCCESS] ETL job: ${job.name} completed.`);
  } catch (error) {
    console.error(`[FAIL] ETL job: ${job.name} failed. Details:`, error);
    throw error;
  }
};

/**
 * BigQuery ë°ì´í„° ê°€ìš©ì„± ì²´í¬
 * íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸ (ê¸°ë³¸ê°’: 1ì¼ ì „)
 */
export const checkDataAvailability = async (targetDate?: string): Promise<boolean> => {
  try {
    const checkDate =
      targetDate ||
      (() => {
        const date = new Date();
        date.setDate(date.getDate() - 1);

        return date.toISOString().split('T')[0];
      })();

    // checkDateë¥¼ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (2025-10-10 -> 20251010)
    const tableSuffix = checkDate.replace(/-/g, '');

    const query = `
      SELECT COUNT(*) as count
      FROM \`${PROJECT_ID}.${RAW_DATASET}.events_*\`
      WHERE _TABLE_SUFFIX = @tableSuffix
    `;

    const [rows] = await bigqueryClient.query({
      query,
      params: { tableSuffix },
    });

    const count = rows[0]?.count || 0;

    void log.info(
      `ë°ì´í„° ê°€ìš©ì„± ì²´í¬ ì™„ë£Œ - KST ${checkDate} ì´ë²¤íŠ¸ ìˆ˜: ${count}`,
      'ANALYTICS',
      'LOW',
      {
        count,
        checkDate,
        rawDataset: RAW_DATASET,
      }
    );

    return count > 0;
  } catch (error) {
    void log.error('ë°ì´í„° ê°€ìš©ì„± ì²´í¬ ì‹¤íŒ¨', 'ANALYTICS', 'MEDIUM', {
      error: error instanceof Error ? error.message : 'Unknown error',
      targetDate,
      rawDataset: RAW_DATASET,
    });
    return false;
  }
};
