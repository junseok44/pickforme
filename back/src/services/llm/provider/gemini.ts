// ì„¤ì¹˜: npm install @google/genai
import { GoogleGenAI, Content, Part, HarmCategory, HarmBlockThreshold } from '@google/genai';
import { ContentPart } from '../ai.provider';

const DEFAULT_MODEL = 'gemini-2.0-flash';

export interface GeminiMessage {
  role: 'user' | 'model'; // GeminiëŠ” user/model ì—­í• ë§Œ ì§€ì›
  content: string | ContentPart[];
}

export class GeminiProvider {
  private ai: GoogleGenAI;

  constructor() {
    const googleApiKey = process.env.GOOGLE_AI_API_KEY;
    if (!googleApiKey) {
      throw new Error('GOOGLE_AI_API_KEY environment variable is required');
    }
    // Helicone API í‚¤ í™•ì¸ ì¶”ê°€
    const heliconeApiKey = process.env.HELICONE_API_KEY;
    if (!heliconeApiKey) {
      throw new Error('HELICONE_API_KEY environment variable is required');
    }

    // GoogleGenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œ Helicone í”„ë¡ì‹œ ì„¤ì • ì¶”ê°€
    this.ai = new GoogleGenAI({
      apiKey: googleApiKey,
      httpOptions: {
        baseUrl: 'https://gateway.helicone.ai',
        headers: {
          'Helicone-Auth': `Bearer ${heliconeApiKey}`,
          'Helicone-Target-URL': 'https://generativelanguage.googleapis.com',
        },
      },
    });
  }

  /**
   * í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í†µí•© Gemini ì‘ë‹µ ìƒì„± ë©”ì„œë“œ
   */
  async generate(params: {
    messages: GeminiMessage[];
    modelName?: string;
    systemInstruction?: string;
  }): Promise<string> {
    const { messages, modelName = DEFAULT_MODEL, systemInstruction } = params;

    if (!messages || messages.length === 0) {
      throw new Error('Messages array cannot be empty.');
    }

    // ë©”ì‹œì§€ë¥¼ Gemini Content í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    const contents: Content[] = messages.map((msg) => {
      if (typeof msg.content === 'string') {
        // ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë©”ì‹œì§€
        return {
          role: msg.role,
          parts: [{ text: msg.content }],
        };
      } else {
        // ë³µí•© ì½˜í…ì¸  (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        const parts: Part[] = msg.content.map((part) => {
          if (part.type === 'text') {
            return { text: part.text || '' };
          } else if (part.type === 'image') {
            return {
              inlineData: {
                mimeType: 'image/jpeg',
                data: part.image || '',
              },
            };
          }
          throw new Error(`Unsupported content part type: ${part.type}`);
        });

        return {
          role: msg.role,
          parts,
        };
      }
    });

    try {
      const response = await this.ai.models.generateContent({
        model: modelName,
        contents,
        // ğŸ‘‡ ì‹œìŠ¤í…œ ì•ˆë‚´ëŠ” ì´ê³³ config ê°ì²´ ë‚´ì— ë³„ë„ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
        config: {
          ...(systemInstruction && { systemInstruction: { parts: [{ text: systemInstruction }] } }),
          thinkingConfig: {
            thinkingBudget: 0,
          },
          temperature: 0.7,
          maxOutputTokens: 2048,
          safetySettings: [
            {
              category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_HARASSMENT,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
              category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
              threshold: HarmBlockThreshold.BLOCK_NONE,
            },
          ],
        },
      });

      return response.text?.trim() || '';
    } catch (error) {
      const errorMsg = `Gemini API call failed: ${error instanceof Error ? error.message : String(error)}`;
      console.error(errorMsg);
      throw new Error(errorMsg);
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì‹œ (í•„ìš”ì‹œ ì‚¬ìš©)
   */
  async *getGeminiStream(prompt: string, modelName: string = 'gemini-1.5-flash-latest') {
    const request = {
      model: modelName,
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
    };

    const stream = await this.ai.models.generateContentStream(request);

    for await (const chunk of stream) {
      yield chunk.text;
    }
  }
}
